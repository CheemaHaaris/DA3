---
title: "DA3 -A2- Technical Report"
author: "Haaris Afzal Cheema"
output:
  html_document:
    rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggplot2)
library(modelsummary)
library(fixest)
library(caret)
library(gridExtra)
library(data.table)
library(ranger)
library(pdp)
library(gbm)
library(rattle)
library(gridExtra)
library(kableExtra)

#### Importing the data ####

df <- read_csv(url('https://raw.githubusercontent.com/CheemaHaaris/DA3/main/Assignment2/Data/listings.csv?token=GHSAT0AAAAAABQS6MPWAM6AGGKJV3A2OZ46YQNL3AA'))

```

# Introduction

This analysis will focus on predicting the price of airbnb apartments in Porto to help a company price their new apartments. These apartments accommodate between 2 to 6 people. Different prediction models of varying complexity will be analyzed and compared. OLS Linear Regressions, Lasso Regression, CART algorithm, Random forest and GBM will be used to gauge the better predictions.

# Sample Design and Feature Engineering

```{r, include=FALSE, message=FALSE, warning=FALSE}
# Subsetting data for number of guests

df <- df %>% filter( accommodates >= 2 & accommodates <=6)


#### Factored Variables ####

# Property Type 

check <- data.table(table(df$property_type))

check <- check[order(-N)]

check

df <- df %>% filter(property_type %in% c("Entire rental unit", "Private room in rental unit",
                                         "Entire residential home", "Private room in residential home",
                                         "Entire loft", "Entire condominium (condo)", 
                                         "Entire serviced apartment"))

df <- df %>% mutate( f_property_type = factor(property_type) )

# neighbourhood_cleansed

df <- df %>% mutate( f_neighbourhood_cleansed = factor(neighbourhood_cleansed) )

# room type

table(df$room_type)  

df <- df %>% mutate(f_room_type = factor(room_type))

############################################################################################################################

#### Numeric Variables ####


# bathrooms - extracting from text var

df <- df %>% mutate(
                     bathroom = factor(gsub( " .*$", "", df$bathrooms_text)))

df <- df %>% filter( bathroom != "Half-bath" & bathroom != "Private")

df$bathroom <- as.numeric(df$bathroom)


# add new numeric columns from certain columns

numericals <- c("accommodates","bathroom","bedrooms",
                "beds","number_of_reviews","review_scores_rating", "review_scores_accuracy",
                "review_scores_cleanliness", "review_scores_checkin", "review_scores_communication",
                "review_scores_location", "review_scores_value","reviews_per_month","minimum_nights")

df <- df %>%
        mutate_at(vars(all_of(numericals)), lst("n"=as.numeric))


# rename columns so they start with n_ as opposed to end with _n
nnames <- df %>%
  select(ends_with("_n")) %>%
  names()
nnames_i <- match(nnames, colnames(df))
colnames(df)[nnames_i] <- paste0("n_", numericals)

#create days since first review

df <- df %>%
  mutate(
          n_days_since = as.numeric(as.Date(calendar_last_scraped,format="%Y-%m-%d") -
                                      as.Date(first_review ,format="%Y-%m-%d")))

############################################################################################################################

#### Binary Variables ####

# binaries # has_availability, instant_bookable

df <- df %>% mutate(
                    d_instant_bookable = ifelse(instant_bookable == 'TRUE', 1, 0),
                    d_superhost = ifelse(host_is_superhost == 'TRUE',1,0))



# binaries 

# identifying the relevant amenities

# extracting the amenities into a column using regex, string splitting and unlisting

amenities <- strsplit(df$amenities,',') %>% unlist() 

amenities <- gsub("[^a-zA-Z]", "", amenities)

amenities <- data.table(unique(amenities))

amenities <- amenities[order(unique(amenities))]

df$amenities <- strsplit(df$amenities, ',')
df$amenities <- tolower(gsub("[^a-zA-Z]", "", df$amenities))

# Testing proportions of relevant amenities

#table(grepl("oven|stove", df$amenities))
#table(grepl("apple tv|netflix|chromecast", df$amenities))
#table(grepl("centralheating|centralairconditioning", df$amenities)) 
#table(grepl("clothing", df$amenities))
#table(grepl("workspace|desk", df$amenities))
#table(grepl("wifi", df$amenities)) 
#table(grepl("kitchen", df$amenities)) 
#table(grepl("pool|hottub", df$amenities))
#table(grepl("gym", df$amenities)) 
#table(grepl("waterfront", df$amenities))
#table(grepl("refrigerator", df$amenities))
#table(grepl("beachfront", df$amenities))
#table(grepl("game", df$amenities))

amenity <- list( 
                oven_stove = "oven|stove",
                streaming = "apple tv|netflix|chromecast",
                central_conditioning = "centralheating|centralairconditioning",
                desk = "workspace|desk",
                wifi = "wifi",
                kitchen = "kitchen",
                pool = "pool|hottub",
                gym = "gym",
                waterfront = "waterfront|beachfront",
                refrigerator = "refrigerator",
                game = "game" )
 
for(i in names(amenity)) df[[i]] <- ifelse(grepl(amenity[[i]], df$amenities)==TRUE,1,0)

# renaming dummy vars

dummies <- names(df)[seq(97,107)]
df <- df %>%
            mutate_at(vars(all_of(dummies)), list("d"= as.numeric))


dnames <- df %>%
  select(ends_with("_d")) %>%
  names()
dnames_i <- match(dnames, colnames(df))
colnames(df)[dnames_i] <- paste0("d_", tolower(gsub("[^[:alnum:]_]", "",dummies)))


######################################################################################################################

#### Price (Target Variable) ####

df <- df %>% mutate( eur_price_day = as.numeric(str_sub(price, 2)))

######################################################################################################################

#### Selecting the relevant variables ####


# keep columns if contain d_, n_,f_, eur_ 
df <- df %>%
  select(matches("^d_.*|^n_.*|^f_.*|^eur_.*"))
```


The initial data contained 10747 observations, which contained the price for the Airbnb apartments for 9th December 2021. The price per day was maintained in Euros. The distribution of the daily price was strongly skewed and the distribution of its log was close to normally distributed.The summary and visuals can be seen below.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# target var

P95 <- function(x){ quantile(x,.95,na.rm=T)}

datasummary( eur_price_day ~ Mean + Median + Min + Max + P25 + P75 + P95 , data = df )

df <- df %>%
            mutate(ln_price = log(eur_price_day))

h1 <-ggplot(data = df) + 
  geom_histogram( aes( x = eur_price_day ) , fill = 'navyblue', col = 'black', alpha = 0.8)  +
  labs( x = "Price (Euros)", y = "Relative Frequency") + theme_bw()

h2 <- ggplot(data = df) + 
  geom_histogram( aes( x = ln_price ) , fill = 'navyblue', col = 'black', alpha = 0.8)  +
  labs( x = "ln (Price, Euros)", y = "Relative Frequency") + theme_bw()

grid.arrange(h1, h2, nrow = 1)

######################################################################################################################

# numerics # lowess with target

l1 <- ggplot(data = subset(df, eur_price_day < 600), aes(x= n_accommodates, y= eur_price_day)) +
  geom_smooth(method="loess", formula = y ~ x)  +
  geom_point(col="navyblue") +
  labs(x = "Number of people accommodated",y = "Price per day (Euros)") + theme_bw()

l2 <- ggplot(data = subset(df, eur_price_day < 600), aes(x= n_beds, y= eur_price_day)) +
  geom_smooth(method="loess", formula = y ~ x)  +
  geom_point(col="navyblue") +
  labs(x = "Number of beds",y = "Price per day (Euros)") + theme_bw()

l3 <- ggplot(data = subset(df, eur_price_day < 600), aes(x= n_bedrooms, y= eur_price_day)) +
  geom_smooth(method="loess", formula = y ~ x)  +
  geom_point(col="navyblue") +
  labs(x = "Number of bedrooms",y = "Price per day (Euros)") + theme_bw()

l4 <- ggplot(data = subset(df, eur_price_day < 600), aes(x= n_number_of_reviews, y= eur_price_day)) +
  geom_point(col="navyblue") +
  geom_smooth(method="loess", formula = y ~ x)  +
  labs(x = "Number of reviews",y = "Price per day (Euros)") + theme_bw()

l5 <- ggplot(data = subset(df, eur_price_day < 600), aes(x= n_reviews_per_month, y= eur_price_day)) +
  geom_point(col="navyblue") +
  geom_smooth(method="loess", formula = y ~ x)  +
  labs(x = "Number of reviews per month",y = "Price per day (Euros)") + theme_bw()

l6 <- ggplot(data = subset(df, eur_price_day < 600), aes(x= n_days_since, y= eur_price_day)) +
  geom_point(col="navyblue") +
  geom_smooth(method="loess", formula = y ~ x)  +
  labs(x = "Number of days since first review",y = "Price per day (Euros)") + theme_bw()



##########################################################################################################################
# Squares and further values to create

df <- df %>%
            mutate(n_accommodates2=n_accommodates^2, 
                   beds2= (n_beds)^2,
                   bedrooms2=n_bedrooms^2,
                   ln_number_of_reviews = log(n_number_of_reviews + 1),
                   ln_reviews_per_month = log(n_reviews_per_month + 1)
            )


# Factorising Discrete Numeric Variables

# checking distributions and applying cutoffs to convert to factored var
#datasummary(factor(n_bathroom) ~ N , data = df  )

df <- df %>%
  mutate( f_beds = cut(n_beds, c(1,2,3,4,12), labels=c(1,2,3,4), right = F),
          f_bedrooms = cut(n_bedrooms, c(1,2,3,10), labels=c(1,2,3), right = F),
          f_bathroom = cut(n_bathroom, c(1,2,3,11), labels=c(1,2,3), right = F),
          f_number_of_reviews = cut(n_number_of_reviews, c(0,10,50,733), labels=c(0,1,2), right = F),
          f_minimum_nights= cut(n_minimum_nights, c(1,2,3,1124),
                                labels=c(1,2,3), right = F))


# factored vars

#datasummary(f_property_type ~ N , data = df  )

#datasummary(f_neighbourhood_cleansed ~ N , data = df  )

#datasummary(f_room_type ~ N , data = df  )


# Change Infinite values with NaNs
for (j in 1:ncol(df) ) data.table::set(df, which(is.infinite(df[[j]])), j, NA)

###############################################################################################################


```


In this case however, we will proceed with taking level price for clearer interpretability of the results. 

As far as predictor variables were concerned, the intuitively more important ones included property type, room type, neighbourhood, along with numerical variables containing information regarding the number of beds, bedrooms and bathrooms. Furthermore, review-based columns pertaining to the number of reviews, reviews per month and the review scores based on criterion such as cleanliness, location etc were also included.

For categorical variables the categories with sufficient representation in the data were kept. In the case of property type, an additional criteria was that categories which come under term of 'apartments' were kept and the rest were filtered out. There were six categories for the property type variable. None of them was clubbed together due to each belonging to a distinct type of apartment. Moreover each category also had sufficient representation so it was decided not to club together any categories. The discrete quantitative variables such as the number of beds, bathrooms, bedrooms etc were analyzed and converted to categorical variables where categories with few values were clubbed together. Next, for the review-based variables, roughly 13% percent values were missing. An interesting find however, in this case was that the values for all these variables were missing in the same rows. So from a sample design decision standpoint, two distinct datasets were created. One, where a combined flag variable was included, and the missing values for each variable were imputed with the median. In the second data, all these rows were dropped (N=1062). The resultant data in this contained 7236 observations as opposed to the other data which contained 8338 observations. This was done, so that the two datasets could be tested in the prediction models separately, and the ones with the better predictions could be kept, whereas the other data would be discarded from the further analysis. Lastly, we had binary variables relating to whether the host was a superhost and whether the apartment was instantly bookable along with a set of amenities which were parsed out of a text variable from the data. The list of amenities was analyzed, and using domain knowledge, 11 amenities were added as binary variables. 

For interactions, conditional distributions of price were looked at based on different values for the categorical variables. Some interactions were included for the neighbourhood variables based on domain knowledge via external research. As can be seen from the distributions below, different price patterns can be observed for different categories based on variations in other predictor variables.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Testing for interactions

# interacting n_accommodates with property type
gg2 <- ggplot(df, aes(x = factor(n_accommodates), y = eur_price_day,
               fill = f_property_type, color= f_property_type)) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  labs(x = "Accomodates (Persons)",y = "Price (Euros)") +
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 250), breaks = seq(0,250, 50)) +
  theme_bw() 

# interacting property type with room type

gg3 <- ggplot(data = subset(df ,eur_price_day < 600), aes(x = factor(f_room_type), y = eur_price_day,
                                                   fill = f_property_type, color= f_property_type)) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  labs(x = "Room Type",y = "Price (Euros)") +
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 250), breaks = seq(0,250, 50)) +
  theme_bw()  

# interacting room type with super host

gg4 <- ggplot(df, aes(x = factor(d_superhost), y = eur_price_day,
               fill = f_room_type, color= f_room_type)) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  labs(x = "Super host",y = "Price (Euros)") +
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 250), breaks = seq(0,250, 50)) +
  theme_bw() 

#  interacting property type with super host

gg5 <- ggplot(df, aes(x = factor(d_superhost), y = eur_price_day,
               fill = f_property_type, color= f_property_type)) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  labs(x = "Super host",y = "Price (Euros)") +
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 250), breaks = seq(0,250, 50)) +
  theme_bw() 

# interacting instant bookable with room type

gg6 <- ggplot(df, aes(x = factor(d_instant_bookable), y = eur_price_day,
               fill =f_room_type , color= f_room_type)) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  labs(x = "Instant Bookable",y = "Price (Euros)") +
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 250), breaks = seq(0,250, 50)) +
  theme_bw() 


# interacting instant bookable with property type

gg7 <- ggplot(df, aes(x = factor(d_instant_bookable), y = eur_price_day,
               fill = f_property_type , color= f_property_type)) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  labs(x = "Instant bookable",y = "Price (Euros)") +
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 250), breaks = seq(0,250, 50)) +
  theme_bw()
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=10}
grid.arrange(gg2,gg3,gg5,gg7, ncol = 1)
```


```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=10}
grid.arrange(gg4,gg6, ncol = 1)

```




```{r, include=FALSE, message=FALSE, warning=FALSE}

##############################################################################################################

# Checking missing values in vars

to_filter <- sapply(df, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

#Problem: same rows missing for review-based columns
#Option 1 - creating a flag and imputing,
#Option 2 - dropping rows to see if better prediction

# dropping rows with no price (target vars)

df <- df %>%
  drop_na(eur_price_day)

# imputing for variables

df <- df %>%
  mutate(
    n_beds = ifelse(is.na(n_beds), n_accommodates, n_beds),
    n_bedrooms =  ifelse(is.na(n_bedrooms), median(n_bedrooms, na.rm = T), n_bedrooms), 
    f_beds=ifelse(is.na(f_beds),1, f_beds),
    f_bathroom=ifelse(is.na(f_bathroom),1, f_bathroom),
    f_bedrooms=ifelse(is.na(f_bedrooms),1, f_bedrooms),
    f_number_of_reviews=ifelse(is.na(f_number_of_reviews),1, f_number_of_reviews),
    f_minimum_nights = ifelse(is.na(f_minimum_nights),1,f_minimum_nights))


# filtering for option 2
df_new <- df %>%
  drop_na(n_review_scores_checkin)

check2 <- df %>% filter(is.na(n_review_scores_checkin))

to_filter <- sapply(check2, function(x) sum(is.na(x)))
to_filter[to_filter > 0]  

# all missing values in df are present in the rows containing missing for n_review_scores_checkin


############################################################################################################
# Option 1

df <- df %>%
  mutate(
    flag_days_since=ifelse(is.na(n_days_since),1, 0),
    n_days_since =  ifelse(is.na(n_days_since), median(n_days_since, na.rm = T), n_days_since),
    flag_reviews_per_month=ifelse(is.na(n_reviews_per_month),1, 0),
    n_reviews_per_month =  ifelse(is.na(n_reviews_per_month), median(n_reviews_per_month, na.rm = T), n_reviews_per_month),
    ln_reviews_per_month =  ifelse(is.na(ln_reviews_per_month), log(median(n_reviews_per_month, na.rm = T)), ln_reviews_per_month),
    flag_review_scores_rating = ifelse(is.na(n_review_scores_rating),1, 0),
    n_review_scores_rating =  ifelse(is.na(n_review_scores_rating), median(n_review_scores_rating, na.rm = T), n_review_scores_rating),
    flag_reviews_combined = ifelse(is.na(n_review_scores_checkin),1, 0),
    n_review_scores_accuracy = ifelse(is.na(n_review_scores_accuracy), median(n_review_scores_accuracy, na.rm = T), n_review_scores_accuracy),
    n_review_scores_cleanliness = ifelse(is.na(n_review_scores_cleanliness), median(n_review_scores_cleanliness, na.rm = T), n_review_scores_cleanliness),
    n_review_scores_checkin = ifelse(is.na(n_review_scores_checkin), median(n_review_scores_checkin, na.rm = T), n_review_scores_checkin),
    n_review_scores_communication = ifelse(is.na(n_review_scores_communication), median(n_review_scores_communication, na.rm = T), n_review_scores_communication),
    n_review_scores_location = ifelse(is.na(n_review_scores_location), median(n_review_scores_location, na.rm = T), n_review_scores_location),
    n_review_scores_value = ifelse(is.na(n_review_scores_value), median(n_review_scores_value, na.rm = T), n_review_scores_value),
    d_superhost = ifelse(is.na(d_superhost), 0,d_superhost),
    beds2 = ifelse(is.na(beds2),median(n_beds)^2, beds2),
    bedrooms2 = ifelse(is.na(bedrooms2),median(n_bedrooms)^2, bedrooms2)
                                   )

to_filter <- sapply(df, function(x) sum(is.na(x)))
to_filter[to_filter > 0]   



#######################################################################################################


#ggplot(data = subset(df, eur_price_day < 600), aes(x= n_review_scores_rating, y= eur_price_day)) +
# geom_point(col="navyblue") +
#  geom_smooth(method="loess", formula = y ~ x)  +
#  labs(x = "Review Score Rating",y = "Price per day (Euros)") + theme_bw()

# redo features
# Create variables, measuring the time since: squared, cubic, logs
df <- df %>%
  mutate(
    n_days_since2=n_days_since^2,
    n_days_since3=-n_days_since^3,
    ln_review_scores_rating = log(n_review_scores_rating),
    ln_review_scores_accuracy = log(n_review_scores_accuracy),
    ln_review_scores_cleanliness = log(n_review_scores_cleanliness),
    ln_review_scores_checkin = log(n_review_scores_checkin),
    ln_review_scores_communication = log(n_review_scores_communication),
    ln_review_scores_location = log(n_review_scores_location),
    ln_review_scores_value = log(n_review_scores_value)
  )

####################################################################################################################

# Option 2 - checking missing

to_filter <- sapply(df_new, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

df_new <- df_new %>%  mutate(d_superhost = ifelse(is.na(d_superhost),0, d_superhost),
                             beds2 = ifelse(is.na(beds2),median(n_beds)^2, beds2),
                             bedrooms2 = ifelse(is.na(bedrooms2),median(n_bedrooms)^2, bedrooms2))

# redo features
# Create variables, measuring the time since: squared, cubic, logs
df_new <- df_new %>%
  mutate(
    n_days_since2=n_days_since^2,
    n_days_since3=-n_days_since^3,
    ln_review_scores_rating = log(n_review_scores_rating),
    ln_review_scores_accuracy = log(n_review_scores_accuracy),
    ln_review_scores_cleanliness = log(n_review_scores_cleanliness),
    ln_review_scores_checkin = log(n_review_scores_checkin),
    ln_review_scores_communication = log(n_review_scores_communication),
    ln_review_scores_location = log(n_review_scores_location),
    ln_review_scores_value = log(n_review_scores_value)
  )


#####################################################################################################################
# copy a variable - purpose later, see at variable importance
df <- df %>% mutate(n_accommodates_copy = n_accommodates)
df_new <- df_new %>% mutate(n_accommodates_copy = n_accommodates)

```


# Model Building


Three regression models were specified for the prediction analysis. The three models differed in terms of model complexity (based  on the number of variables) where the first model contained the key predictor variables. The second contained the review-based variables as well as the binary variables in addition to the basic key predictors. For the last, most complex model, interaction terms of the neighbourhood with property and room type along with number of people accommodated were added. These regression models can be seen below:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
m1 <- "= Number of guests accommodated (squared term also), number of beds (squared term also), number of bedrooms (squared term also), Days since first review (squared and cubic also), Neighbourhood, Minimum nights of stay"

m2 <- "= m1 + log of all review based columns + relevant amenities"

m3 <- "= m2 + selected interaction terms with prop type and room type + interaction terms with neighbourhoods"

m_variables <- c(m1,m2,m3)
m_names <- c("M1", "M2","M3")
m_table <- as.data.frame(cbind(m_names, m_variables))
m_headings <- c("Model", "Predictor Variables")
colnames(m_table) <- m_headings
```


```{r, echo=FALSE, message=FALSE, warning=FALSE}
m_table %>%
  kbl(caption = "Models on which algorithms will be run", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center")

```

We looked at patterns of association between the numerical predictor variables and the target variable. It was found that the price is approximately linearly related to the number of people that the apartment can accommodate. However, due to a slightly non-linear trend for higher values of this predictor variable, it was specified as a quadratic. Parabolas were observed for the number of beds, bedrooms and the days since the first review. These were modelled as quadratics as well. For review-linked variables, non-linear trends were observed for which their logarithms were included. A summary of the trends observed can be seen below:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
grid.arrange(l1,l2,l3,l4,l5,l6, ncol = 3)

```


For model training and testing, the sample was divided into two sub-samples, namely the work sample and the holdout sample. The work sample contained 70% of randomly selected observations from the initial sample whereas 30% were kept in the holdout set. 5-fold cross validation was done on the work set, where 5 further random splits were made (after replacement)for the training set and test usage. The code snippet for this process can be seen below. Because of the sample design dilemma, two sets of train and holdout sets were created. Similarly, two sets of basic variables and review-based variables were created. In the dataset with imputed values, additional flag variables were created. Based on domain knowledge, as well as the exploratory analysis done above for interactions, sets of interactions were created to be included in the more complex models. The set X3 was created to interact the binary flag variable with other key variables in case that was the dataset which produces the better prediction.



```{r, message=FALSE, warning=FALSE}

set.seed(2801)
train_indices <- as.integer(createDataPartition(df$eur_price_day, p = 0.7,
                                                list = FALSE))
data_train <- df[train_indices, ]
data_holdout <- df[-train_indices, ]

train_indices2 <- as.integer(createDataPartition(df_new$eur_price_day, p = 0.7,
                                                 list = FALSE))
data_train2 <- df_new[train_indices2, ]
data_holdout2 <- df_new[-train_indices2, ]

# Basic Variables inc neighbourhood - df
basic_vars <- c(
  "n_accommodates","n_accommodates2", "beds2","bedrooms2","n_beds","n_bedrooms",
  
  "n_days_since","f_property_type","f_room_type", "n_bathroom","n_bedrooms",
  "n_minimum_nights",
  "f_neighbourhood_cleansed","flag_days_since","n_days_since2","n_days_since3")

# Basic Variables inc neighbourhood - df_new
basic_vars2 <- c(
  "n_accommodates","n_accommodates2","n_beds","n_bedrooms","beds2","bedrooms2",
  "n_days_since","f_property_type","f_room_type", "n_bathroom", 
  "n_minimum_nights","f_neighbourhood_cleansed","n_days_since2","n_days_since3")

# reviews - df
reviews <- c("flag_reviews_combined" , 
             "ln_number_of_reviews", "ln_reviews_per_month",
             "ln_review_scores_rating","ln_review_scores_accuracy",
             "ln_review_scores_cleanliness",
             "ln_review_scores_checkin","ln_review_scores_communication",
             "ln_review_scores_location",
             "ln_review_scores_value","flag_review_scores_rating")

# reviews - df_new
reviews2 <- c("ln_review_scores_rating","ln_review_scores_accuracy",
              "ln_number_of_reviews","ln_reviews_per_month",
             "ln_review_scores_cleanliness","n_review_scores_checkin",
             "ln_review_scores_communication","n_review_scores_location",
             "ln_review_scores_value")

# Dummy variables
binaries <-  grep("^d_.*", names(df), value = TRUE)

#interactions for the LASSO

X1  <- c("n_accommodates*f_property_type",  "f_room_type*f_property_type",
         "f_room_type*d_superhost","d_superhost*f_property_type",
         "d_instant_bookable*f_room_type", "d_instant_bookable*f_property_type")

# interacting with neighborhoods

X2  <- c("f_property_type*f_neighbourhood_cleansed", 
         "f_room_type*f_neighbourhood_cleansed",
         "n_accommodates*f_neighbourhood_cleansed" )

# interacting with combined flag

X3  <- c("flag_reviews_combined*f_property_type",
         "flag_reviews_combined*f_room_type",
         "flag_reviews_combined*f_neighbourhood_cleansed",
         "flag_reviews_combined*n_accommodates",
         "flag_reviews_combined*n_review_scores_rating", 
         "flag_reviews_combined*n_reviews_per_month")


predictors_1 <- c(basic_vars)
predictors_2 <- c(basic_vars, reviews, binaries)
predictors_E <- c(basic_vars2, reviews2, binaries, X1,X2)

predictors_1_new <- c(basic_vars2)
predictors_2_new <- c(basic_vars2, reviews2, binaries)

```




# Model Evaluation


```{r, include=FALSE, message=FALSE,warning=FALSE}
#########################################################################################
#
# PART II
# RANDOM FORESTS -------------------------------------------------------
#
# We are going to make some simplification for faster running time
#   see the original codes on the book's github page!
#

# do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)

# set tuning
tune_grid <- expand.grid(
  .mtry = c(8),
  .splitrule = "variance",
  .min.node.size = c(50)
)

########################## Random forest - df ##################################
# simpler model for model - using random forest
set.seed(1234)
system.time({
  rf_model_1 <- train(
    formula(paste0("eur_price_day ~", paste0(predictors_1_new, collapse = " + "))),
    data = data_train2,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )
})
rf_model_1

# more complicated model - using random forest
set.seed(1234)
system.time({
  rf_model_2 <- train(
    formula(paste0("eur_price_day ~", paste0(predictors_2_new, collapse = " + "))),
    data = data_train2,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )
})
rf_model_2

######################### Random Forest - df_new ##############################

# simpler model for model - using random forest
set.seed(1234)
system.time({
  rf_model_3 <- train(
    formula(paste0("eur_price_day ~", paste0(predictors_1, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )
})
rf_model_3

# more complicated model - using random forest
set.seed(1234)
system.time({
  rf_model_4 <- train(
    formula(paste0("eur_price_day ~", paste0(predictors_2, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )
})
rf_model_4


# evaluate random forests -------------------------------------------------

results <- resamples(
  list(
    model_1  = rf_model_1,
    model_2  = rf_model_2,
    model_3 = rf_model_3,
    model_4 = rf_model_4
  )
)
summary(results)



#########################################################################################
#
# PART III
# MODEL DIAGNOSTICS -------------------------------------------------------
#


#########################################################################################
# Variable Importance Plots -------------------------------------------------------
#########################################################################################

# variable importance plot
# 1) full varimp plot, full
# 2) varimp plot grouped
# 3) varimp plot , top 10
# 4) varimp plot  w copy, top 10


rf_model_2_var_imp <- ranger::importance(rf_model_2$finalModel)/1000
rf_model_2_var_imp_df <-
  data.frame(varname = names(rf_model_2_var_imp),imp = rf_model_2_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Borough:", varname) ) %>%
  mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_2_var_imp_df
##############################
# 1) full varimp plot, above a cutoff
##############################

# to have a quick look
plot(varImp(rf_model_2))
rf_model_2_var_imp_df$imp

cutoff = 100
ggplot(rf_model_2_var_imp_df[rf_model_2_var_imp_df$imp>cutoff,],
       aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1.5) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=1) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw() +
  theme(axis.text.x = element_text(size=6), axis.text.y = element_text(size=6),
        axis.title.x = element_text(size=6), axis.title.y = element_text(size=6))




##############################
# 2) varimp plot grouped
##############################
# grouped variable importance - keep binaries created off factors together

varnames <- rf_model_2$finalModel$xNames
f_neighbourhood_cleansed_varnames <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)
f_room_type_varnames <- grep("f_room_type",varnames, value = TRUE)

groups <- list(f_neighbourhood_cleansed=f_neighbourhood_cleansed_varnames,
               f_property_type = f_property_type_varnames,
               f_room_type = f_room_type_varnames,
               n_days_since = "n_days_since",
               n_accommodates = "n_accommodates",
               beds2 = "beds2",
               bedrooms2 = "bedrooms2")

# Need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_2_var_imp_grouped <- group.importance(rf_model_2$finalModel, groups)
rf_model_2_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_2_var_imp_grouped),
                                            imp = rf_model_2_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

####
# Subsample performance: RMSE / mean(y) ---------------------------------------
# NOTE  we do this on the holdout set.

# ---- cheaper or more expensive flats - not used in book
data_holdout_w_prediction <- data_holdout2 %>%
  mutate(predicted_price = predict(rf_model_2, newdata = data_holdout2))



######### create nice summary table of heterogeneity
a <- data_holdout_w_prediction %>%
  mutate(is_low_size = ifelse(n_accommodates <= 3, "small apt", "large apt")) %>%
  group_by(is_low_size) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, eur_price_day),
    mean_price = mean(eur_price_day),
    rmse_norm = RMSE(predicted_price, eur_price_day) / mean(eur_price_day)
  )

############################################################################################

# table(data_holdout_w_prediction$f_neighbourhood_cleansed)

b <- data_holdout_w_prediction %>%
  filter(f_neighbourhood_cleansed %in% c("Santa Marinha e São Pedro da Afurada", "Lordelo do Ouro e Massarelos",
                                         "Mafamude e Vilar do Paraíso", "Matosinhos e Leça da Palmeira")) %>%
  group_by(f_neighbourhood_cleansed) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, eur_price_day),
    mean_price = mean(eur_price_day),
    rmse_norm = RMSE(predicted_price, eur_price_day) / mean(eur_price_day)
  )



c <- data_holdout_w_prediction %>%
          group_by(f_property_type) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, eur_price_day),
    mean_price = mean(eur_price_day),
    rmse_norm = RMSE(predicted_price, eur_price_day) / mean(eur_price_day)
  )


d <- data_holdout_w_prediction %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, eur_price_day),
    mean_price = mean(eur_price_day),
    rmse_norm = RMSE(predicted_price, eur_price_day) / mean(eur_price_day)
  )

# Save output
colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")

line1 <- c("Type", "", "", "")
line2 <- c("Apartment size", "", "", "")
line3 <- c("Borough", "", "", "")

result_3 <- rbind(line2, a, line1, c, line3, b, d) %>%
  transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
            `RMSE/price` = as.numeric(`RMSE/price`))

result_3


```


## Random forest

Firstly, the random forest algorithm was used in the prediction exercise. Prior to running the algorithm, tuning parameters were defined. The number of predictor variables to consider for each split when growing the decorrelated trees was kept at 8. This was done by following the standard practice of using the square root of the number of predictor variables (I rounded up in this case). Next, the stopping rule for each tree was kept with a minimum of 50 observations in each node. Finally, for the number of bootstrap samples, the default option of 500 was chosen. Not a lot of experimentation was done with the tuning parameters due to the robustness of random forest towards the tuning parameters. Due to the sample design dilemma faced earlier, the algorithm was run on two models (model 1 and 2) for each of the two distinct datasets (one with imputation and the other with the dropped rows). It turned out that the average RMSE was lower (47.55) in the case where the rows were dropped as opposed to the data where imputation was done (49.52) when comparing the more complex model (model 2). A similar trend was observed in the simpler models. Therefore, all subsequent predictions were also evaluated on this dataset. 

Because of the 'blackbox' nature of the algorithm, model diagnostics were performed to identify some of the most important variables in RMSE reduction. To get a clearer picture, the top 10 RMSE reducers were visualized and the results can be seen below. Many of the review-based variables along with the number of bedrooms were considerably important variables in improving the fit of the predictions.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
##############################
# 2) full varimp plot, top 10 only
##############################

# have a version with top 10 vars only
ggplot(rf_model_2_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='navyblue', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='navyblue', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()
```


In the case of grouped variables, it turns out that neighbourhood cleansed is the most important in improving the model fit. Property type, number of variables (discrete factored) and the number of people the apartment can accommodate turned out to be key grouped variables which are considerably important in improving the fit of the prediction. 

```{r,echo=FALSE, warning=FALSE, message=FALSE}
ggplot(rf_model_2_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='navyblue', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='navyblue', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

```


Next, a couple of these top variables were explored by examining their impact on the target variable conditioned on other variables. It can indeed be verified that variation in these variables tends to impact the expected price by quite a fair amount.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
##########################
# Partial Dependence Plots 
##########################

# 1) Number of accommodates
pdp_n_acc <- pdp::partial(rf_model_2, pred.var = "n_accommodates", 
                          pred.grid = distinct_(data_holdout2, "n_accommodates"), 
                          train = data_train2)

pdp_n_acc %>%
  autoplot( ) +
  geom_point(color='navyblue', size=2) +
  geom_line(color='navyblue', size=1) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  scale_x_continuous(limit=c(1,7), breaks=seq(1,7,1))+
  theme_bw()


# 2) Bedrooms
pdp_n_roomtype <- pdp::partial(rf_model_2, pred.var = "n_bedrooms", 
                               pred.grid = distinct_(data_holdout2, "n_bedrooms"), 
                               train = data_train2)
pdp_n_roomtype %>%
  autoplot( ) +
  geom_point(color='navyblue', size=4) +
  ylab("Predicted price") +
  xlab("Number of Bedrooms") +
  scale_y_continuous(limits=c(60,72), breaks=seq(60,72, by=2)) +
  theme_bw()
```

```{r, echo=FALSE}
result_3 %>%
  kbl(caption = "<center><strong>Sub-Sample Diagnostics</strong></center>", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center")
```

Finally, the extended rf model was examined using sub-samples pertaining to three variables, namely, the number of people the apartment size (based on n_accommodates), the property type and the neighbourhoods. The neighbourhoods were selected randomly from their appearance on the full variable importance plot. It can be seen that predicting the price for smaller apartments is harder than bigger ones. Similarly, entire rental units have a much higher RMSE:price ratio, and hence are harder to predict than all other property types. Lastly, the neighbourhood, 'Mafamude e Vilar do Paraíso' had substantially higher RMSE:price ratio and is much harder to predict than other neighbourhoods.

## OLS and Lasso

```{r, include=FALSE, message=FALSE, warning=FALSE}
#########################################################################################
#
# PART IV
# HORSERACE: compare with other models -----------------------------------------------
#
#########################################################################################

# OLS 
# using 2nd model

set.seed(1234)
system.time({
  ols_model <- train(
    formula(paste0("eur_price_day ~", paste0(predictors_2_new, collapse = " + "))),
    data = data_train2,
    method = "lm",
    trControl = train_control
  )
})

ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))



# * LASSO
# using extended model w interactions

set.seed(1234)
system.time({
  lasso_model <- train(
    formula(paste0("eur_price_day ~", paste0(predictors_E, collapse = " + "))),
    data = data_train2,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
    trControl = train_control
  )
})

lasso_coeffs <- coef(
  lasso_model$finalModel,
  lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(lasso_coefficient = `s1`)  # the column has a name "1", to be renamed

lasso_coeffs_non_null <- lasso_coeffs[!lasso_coeffs$lasso_coefficient == 0,]

regression_coeffs <- merge(ols_model_coeffs_df, lasso_coeffs_non_null, by = "variable", all=TRUE)

```


OLS linear regression was run on model 2. It was run with a 5-fold CV. 159 variables were used in this case. Similarly, for LASSO, the most complex model (model 3) was used, which had the highest complexity, in terms of the number of variables as well as the functional forms assigned. This algorithm was also run with a 5-fold cross validation for selecting the optimal value of lambda. In the end, the algorithm picked a model with 218 variables. An overlap is observed between the LASSO coefficients and the OLS coefficients, and the values are also not too different. However, different variable and functional form selection is also seen in the case of LASSO.

## CART and GBM

```{r, include=FALSE, message=FALSE, warning=FALSE}

# CART with built-in pruning
set.seed(1234)
system.time({
  cart_model <- train(
    formula(paste0("eur_price_day ~", paste0(predictors_2_new, collapse = " + "))),
    data = data_train2,
    method = "rpart",
    tuneLength = 10,
    trControl = train_control
  )
})
cart_model
# Showing an alternative for plotting a tree
fancyRpartPlot(cart_model$finalModel, sub = "")

# GBM  -------------------------------------------------------

gbm_grid <-  expand.grid(interaction.depth = 5, # complexity of the tree
                         n.trees = 250, # number of iterations, i.e. trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)


set.seed(1234)
system.time({
  gbm_model <- train(formula(paste0("eur_price_day ~", paste0(predictors_2_new, collapse = " + "))),
                     data = data_train2,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)
})
gbm_model
gbm_model$finalModel

```

In the case of the CART algorithm, resampling results were obtained for 10 values of the complexity parameter (in the order of increasingly stricter stopping rules). RMSE was the metric that was chosen to obtain the optimal model out of the 10 samples. The final value used for the model was with the complexity parameter equal to 0.0157, indicating that the stopping rule for the splitting would be where the R-squared in the test sample would increase by less than 1.57%. The regression tree indicates that splitting the data based on the number of bedrooms improves the model fit and reduces the RMSE the most.Similarly, the next split is done on the basis on the roomtype category 'Private room'. To avoid overfitting of the regression tree, pruning of the tree has been added as an input in the model setup.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
fancyRpartPlot(cart_model$finalModel, sub = "")
```


Lastly, GBM was used for the final model, which is similar to random forest, except that instead of growing independent trees, it grows trees which build on each other, and then they are combined to make a prediction. Multiple tuning parameters had to be specified in this case. The tree complexity was set at 5 and the number of trees made were kept at 250. The learning rate was specified at 0.1 and the minimum samples were 20.


```{r, echo=FALSE, message=FALSE, warning=FALSE}

final_models <-
  list("OLS" = ols_model,
       "LASSO (model w/ interactions)" = lasso_model,
       "CART" = cart_model,
       "Random forest 1: smaller model" = rf_model_1,
       "Random forest 2: extended model" = rf_model_2,
       "GBM"  = gbm_model)

results <- resamples(final_models) %>% summary()

# Model selection is carried out on this CV RMSE
result_4 <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")

result_4 %>% kbl(caption = "Horse Race of Models CV RSME", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center")

```


For all models, the cross validated RMSE was calculated and the results are shown below. Random forest on the extended model has the lowest RMSE in this case, indicating the best fit for the prediction. That is followed by the GBM method which also produces a good fit for the prediction. The last contender in this case is the CART algorithm with an RMSE equal to 50. 

# Conclusion


```{r, echo=FALSE, message=FALSE, warning=FALSE}
# evaluate preferred model on the holdout set -----------------------------

result_5 <- map(final_models, ~{
  RMSE(predict(.x, newdata = data_holdout2), data_holdout2[["eur_price_day"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")

result_5 %>% kbl(caption = "Holdout set RSME of models", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center")
```


Due to the best performance in the CV RMSE, the random forest extended model is now evaluated on the holdout set. The holdout set RMSE is 50.84 as opposed to the CV RMSE of 47.55. There is a little difference in the CV RMSE and holdout set RMSE, which can partly be attributed to the difference in sample sizes of the train and holdout sets. This means that we can expect to make an error of 50.84 Euros when using our model on live data in case we have high external validity. 